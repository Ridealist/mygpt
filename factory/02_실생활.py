import os
import streamlit as st

from langchain_core.messages.chat import ChatMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser


from langchain_community.chat_message_histories import ChatMessageHistory
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_community.document_loaders import PDFPlumberLoader
# from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI #, OpenAIEmbeddings

from langchain_teddynote.prompts import load_prompt
from langchain_teddynote import logging

st.session_state.api_key = st.secrets["openai_api_key"]

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# ì„¸ì…˜ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
store = {} 


st.title("êµê³¼ì„œ ì±—ë´‡ê³¼ í•¨ê»˜ QAğŸ’¬")
st.text("êµê³¼ì„œì—ì„œ ì˜¤ëŠ˜ ë°°ìš¸ ë‚´ìš©ê³¼ ê´€ë ¨ ìˆëŠ” ë‚´ìš©ì„ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")


# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []


# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    print(session_ids)
    if session_ids not in store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        store[session_ids] = ChatMessageHistory()
    return store[session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜

# # íŒŒì¼ì„ ìºì‹œ ì €ì¥(ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)
# @st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
# def embed_file(file):
#     # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
#     file_content = file.read()
#     file_path = f"./.cache/files/{file.name}"
#     with open(file_path, "wb") as f:
#         f.write(file_content)

#     # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)
#     loader = PDFPlumberLoader(file_path)
#     docs = loader.load()

#     # ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• (Split Documents)
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
#     split_documents = text_splitter.split_documents(docs)

#     # ë‹¨ê³„ 3: ì„ë² ë”©(Embedding) ìƒì„±
#     embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key = st.session_state.api_key)

#     # ë‹¨ê³„ 4: DB ìƒì„±(Create DB) ë° ì €ì¥
#     # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
#     vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

#     # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
#     # ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.
#     retriever = vectorstore.as_retriever()
#     return retriever


# ì²´ì¸ ìƒì„±
def create_chain(model_name="gpt-4o"):
    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ëŒ€í™”í˜• í•™ìŠµì„ ë•ëŠ” **ë¬¼ë¦¬ íŠœí„°**ì…ë‹ˆë‹¤.
**ëª©í‘œ:** í•™ìƒì´ 'ë“±ì† ì›ìš´ë™'ì˜ ê°œë…ê³¼ 'ì¤‘ì‹¬ ë°©í–¥ìœ¼ë¡œ ì‘ìš©í•˜ëŠ” í˜(êµ¬ì‹¬ë ¥)'ì˜ í•„ìš”ì„±ì„ ì´í•´í•˜ë„ë¡ ë•ëŠ” ê²ƒì…ë‹ˆë‹¤.  
 
1. ì‹¤ìƒí™œì—ì„œ ì›ìš´ë™ê³¼ ê´€ë ¨ëœ ì˜ˆì‹œë¥¼ í•™ìƒì—ê²Œ ë– ì˜¬ë¦¬ê²Œ í•˜ì„¸ìš”.  
2. ì›ìš´ë™ì„ ìœ ì§€í•˜ëŠ” ë° í•„ìš”í•œ ì¡°ê±´ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì§ˆë¬¸í•˜ê³ , í•™ìƒì´ ìŠ¤ìŠ¤ë¡œ íƒêµ¬í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.  
3. 'êµ¬ì‹¬ë ¥'ì´ ì›ìš´ë™ì˜ í•„ìˆ˜ ì¡°ê±´ì„ì„ ì´í•´í•˜ë„ë¡ ëŒ€í™”ë¥¼ ì´ëŒì–´ì£¼ì„¸ìš”.  
 
**ëŒ€í™” ìŠ¤íƒ€ì¼:**  
- í•™ìƒì˜ í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ëŠ” ì—´ë¦° ì§ˆë¬¸ì„ í™œìš©í•˜ì„¸ìš”.  
- í•™ìƒì˜ ë‹µë³€ì„ ê²©ë ¤í•˜ê³ , ë¶€ì¡±í•œ ë¶€ë¶„ì€ ì¶”ê°€ ì§ˆë¬¸ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”.  
- ë³µì¡í•œ ê°œë…ì„ ê°„ë‹¨íˆ ì„¤ëª…í•˜ë˜, í•™ìƒì´ ìŠ¤ìŠ¤ë¡œ ë°œê²¬í•˜ë„ë¡ ê¸°íšŒë¥¼ ì£¼ì„¸ìš”.  
 
**ëŒ€í™” ì‹œì‘ ì œì•ˆ:**  
- "ë¹™ê¸€ë¹™ê¸€ ëŒì•„ê°€ëŠ” ë†€ì´ê¸°êµ¬ íƒ€ë´¤ì–´? ëª¸ì´ ì–´ë–»ê²Œ ëŠê»´ì¡Œì–´?"  
- "ìë™ì°¨ê°€ ê³¡ì„  ë„ë¡œë¥¼ ëŒ ë•Œ ëª¸ì´ ì–´ëŠ ìª½ìœ¼ë¡œ ì ë¦¬ë˜ê°€?"  
- "ê³µì„ ëˆì— ë§¤ë‹¬ê³  ëŒë¦´ ë•Œ, ëˆì´ ì—†ë‹¤ë©´ ê³µì€ ì–´ë–»ê²Œ ë ê¹Œ?"  
 
**í•µì‹¬ ì§ˆë¬¸ ìœ ë„:**  
- "ì™œ ëª¸ì´ ë°”ê¹¥ìª½ìœ¼ë¡œ ì ë¦¬ëŠ” ëŠë‚Œì´ ë“¤ê¹Œ?"  
- "ëˆì´ ì—†ìœ¼ë©´ ê³µì´ ê³„ì† ì›ìš´ë™ì„ í•  ìˆ˜ ìˆì„ê¹Œ? ì™œ ê·¸ëŸ´ê¹Œ?"  
- "ë§Œì•½ ëˆì´ ë‹¹ê¸°ì§€ ì•ŠëŠ”ë‹¤ë©´ ê³µì€ ì–´ë–»ê²Œ ì›€ì§ì¼ê¹Œ?"  
 
**í•µì‹¬ ê°œë… ì„¤ëª… ìœ ë„:**  
- ì›ìš´ë™ì—ì„œ ì†ë„ì˜ ë°©í–¥ì€ ê³„ì† ë°”ë€Œë©°, ì´ëŠ” ê°€ì†ë„(êµ¬ì‹¬ ê°€ì†ë„)ê°€ ë°œìƒí•¨ì„ ì˜ë¯¸í•¨.  
- êµ¬ì‹¬ ê°€ì†ë„ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í˜(êµ¬ì‹¬ë ¥)ì´ ì›ì˜ ì¤‘ì‹¬ ë°©í–¥ìœ¼ë¡œ ì‘ìš©í•´ì•¼ í•¨.  
- êµ¬ì‹¬ë ¥ì´ ì—†ìœ¼ë©´ ë¬¼ì²´ëŠ” ì§ì„  ë°©í–¥(ë“±ì† ì§ì„  ìš´ë™)ìœ¼ë¡œ ì›€ì§ì´ë ¤ í•¨.  
 
**ëŒ€í™” ë§ˆë¬´ë¦¬:**  
í•™ìƒì´ ìì‹ ë§Œì˜ ë§ë¡œ 'ë“±ì† ì›ìš´ë™ì„ ìœ ì§€í•˜ë ¤ë©´ ì¤‘ì‹¬ ë°©í–¥ì˜ í˜ì´ í•„ìš”í•˜ë‹¤'ëŠ” ê°œë…ì„ ì •ë¦¬í•˜ë„ë¡ ë„ì™€ì£¼ì„¸ìš”.  
- ì˜ˆ: "ê·¸ëŸ¼ ë„¤ ë§ë¡œ ì •ë¦¬í•´ë³´ì. ê³µì´ ì›ìš´ë™ì„ í•˜ë ¤ë©´ ì–´ë–¤ í˜ì´ í•„ìš”í•˜ê³ , ê·¸ í˜ì€ ì–´ë””ë¡œ ì‘ìš©í•´ì•¼ í• ê¹Œ?""",
            ),
            # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, history ê°€ MessageHistory ì˜ key ê°€ ë¨
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),  # ì‚¬ìš©ì ì…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©
        ]
    )

    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
    # ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name=model_name, temperature=0, openai_api_key = st.session_state.api_key)
    
    # ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±
    with_message_history = (
        RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
            prompt | llm,
            get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
            input_messages_key="input",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤
            history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
        )
    )
    chain = with_message_history | StrOutputParser()

    return chain


# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if clear_btn:
    st.session_state["messages"] = []


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()


# ì²´ì¸ ìƒì„±
st.session_state["chain"] = create_chain()

if user_input:
    chain = st.session_state["chain"]

    if chain is not None:
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = chain.stream(
            {"input": user_input},
            # ì„¤ì • ì •ë³´ë¡œ ì„¸ì…˜ ID "abc123"ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
            config={"configurable": {"session_id": "abc123"}},
        )
        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
        add_message("user", user_input)
        add_message("assistant", ai_answer)
