import os
import streamlit as st

from langchain_core.messages.chat import ChatMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI #, OpenAIEmbeddings

from langchain_teddynote import logging

st.session_state.api_key = st.secrets["openai_api_key"]

os.environ["LANGCHAIN_TRACING_V2"] = st.secrets["LANGCHAIN_TRACING_V2"]
os.environ["LANGCHAIN_API_KEY"] = st.secrets["LANGCHAIN_API_KEY"]
os.environ["LANGCHAIN_ENDPOINT"] = st.secrets["LANGCHAIN_ENDPOINT"]
os.environ["LANGCHAIN_PROJECT"] = st.secrets["LANGCHAIN_PROJECT"]

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# ì„¸ì…˜ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
store = {} 


st.title("ë°°ìš´ ë‚´ìš© ì ìš©í•˜ê¸° ğŸ“Œ")
st.text("ë°°ìš´ ë‚´ìš©ì„ ìƒˆë¡œìš´ ìƒí™©ì— ì ìš©í•´ë³´ë©° ê°œë…ì„ ë” ê¹Šê²Œ ì´í•´í•´ë´…ì‹œë‹¤.")

st.image("https://bobmoler.wordpress.com/wp-content/uploads/2019/03/orbit_360p30-1.gif", caption="ì§€êµ¬ ì£¼ìœ„ë¥¼ ë„ëŠ” ë‹¬ì˜ ëª¨ìŠµ")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages_application" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages_application"] = []


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages_application"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages_application"].append(ChatMessage(role=role, content=message))

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    print(session_ids)
    if session_ids not in store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        store[session_ids] = ChatMessageHistory()
    return store[session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜



# ì ìš© ì§ˆë¬¸(ë‚˜ì¤‘ì—” DBì—ì„œ ë¶ˆëŸ¬ì˜¤ë©´ ì¢‹ì„ ë“¯)
applyingQuestion = "ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ë‹¬ì´ ê³µì „ ê¶¤ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê³„ì† ì§€êµ¬ë¥¼ ëŒ ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?" 

# ì ìš©í•˜ê¸° ëª¨ë²”ë‹µì•ˆ (ë‚˜ì¤‘ì—” DBì—ì„œ ë¶ˆëŸ¬ì˜¤ë©´ ì¢‹ì„ ë“¯)
applyingModeledAnswer = "ë‹¬ì—ëŠ” ì§€êµ¬ê°€ ì‘ìš©í•œ ì¤‘ë ¥ì´ ê³„ì†í•´ì„œ ì§€êµ¬ ì¤‘ì‹¬ë°©í–¥ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆìœ¼ë©°, ì´ ì¤‘ë ¥ì´ ë‹¬ì˜ ê³µì „ ê¶¤ë„ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ êµ¬ì‹¬ë ¥ì˜ ì—­í• ì„ í•˜ì—¬ ë‹¬ì€ ì§€êµ¬ ì£¼ë³€ì„ ê³„ì† ëŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤." 

# ë‹µë³€ ë¹„êµ ì ìˆ˜
def relevance_check(applyingModeledAnswer, model_name="gpt-4o"):
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a grader assessing the relevance of a student's essay answer to a given model answer. 
        Here is the model answer: {applyingModeledAnswer}

        Consider both the student's current answer and their previous responses in the conversation history to evaluate their understanding.
        
        Evaluation criteria:
        1. Accuracy and relevance compared to the model answer
        2. Progressive improvement in understanding shown through the conversation
        3. Integration of previous responses with new insights
        
        Combine the current input and conversation history to form a complete understanding of the student's answer.
        Then compare this combined answer against the model answer.
        
        Calculate a precise percentage score from 0 to 100 based on the above criteria.
        Be detailed in your scoring - use decimal points if needed (e.g., 67.5, 82.3, etc.).
        Do not round to the nearest 5 or 10.
        
        Return only the number without any additional text or explanation.""",
            ),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),
            ("ai", "Based on both the current answer and previous responses, calculate a precise score from 0-100 with decimal points if needed."),
        ]
    )

    llm = ChatOpenAI(model_name=model_name, temperature=0, openai_api_key = st.session_state.api_key)

    with_message_history = RunnableWithMessageHistory(
        prompt | llm,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )
    chain = with_message_history | StrOutputParser()
    
    # ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ ê²°ê³¼ê°’ì„ ë°›ì•„ì˜´
    score = chain.invoke(
        {"input": user_input, "applyingModeledAnswer": applyingModeledAnswer},
        config={"configurable": {"session_id": "abc123"}}
    )
    
    # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
    try:
        # ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ì°¾ì•„ ë°˜í™˜
        import re
        numbers = re.findall(r'\d+\.?\d*', score)
        if numbers:
            return numbers[-1]  # ë§ˆì§€ë§‰ ìˆ«ì ë°˜í™˜
        else:
            return "0"  # ìˆ«ìë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
    except Exception as e:
        print(f"Error parsing score: {e}")
        return "0"

# ë‹µë³€ ì²´ì¸ ìƒì„±
def create_chain(model_name="gpt-4o"):
    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ëŒ€í™”í˜• í•™ìŠµì„ ë•ëŠ” **ë¬¼ë¦¬ íŠœí„°**ì…ë‹ˆë‹¤.  
**ëª©í‘œ:** í•™ìƒì´ ìƒˆë¡œ ìµíŒ 'ë“±ì† ì›ìš´ë™'ê³¼ ê´€ë ¨ëœ ê°œë…ì„ ì‹¤ì œ ë¬¸ì œ ìƒí™©ì— ì ìš©í•˜ê³ , ì´ë¥¼ í†µí•´ ê°œë…ì˜ ìœ ìš©ì„±ì„ íƒêµ¬í•˜ë©° ë°˜ì„±ì  ì‚¬ê³ ë¥¼ í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.

**ì¤‘ìš”:**  
1. í•™ìƒì´ ìƒˆë¡œ ìµíŒ ê°œë…ì„ ë¬¸ì œ í’€ì´ì— ì ìš©í•˜ë„ë¡ ê²©ë ¤í•˜ì„¸ìš”.  
2. í•™ìƒì´ ìì‹ ì˜ ë‹µë³€ì„ ì„±ì°°í•˜ë©° ìƒˆë¡œìš´ ê°œë…ì´ ê¸°ì¡´ ì„ ê°œë…ê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ìŠ¤ìŠ¤ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ìœ ë„í•˜ì„¸ìš”.  
3. ì§ì ‘ì ì¸ ì •ë‹µì„ ì œê³µí•˜ì§€ ë§ê³ , í•™ìƒì´ ìŠ¤ìŠ¤ë¡œ ê°œë…ì„ í™œìš©í•˜ì—¬ ë‹µì„ êµ¬ì„±í•˜ë„ë¡ ë„ì™€ì£¼ì„¸ìš”.  
4. í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ ëŒ€í™”ë¥¼ ìœ ì§€í•˜ì„¸ìš”.  
5. ë‘ ë¬¸ì¥ì„ ë„˜ì§€ ì•Šê²Œ ëŒ€í™”ë¥¼ ìƒì„±í•˜ì„¸ìš”.
6. í•™ìƒì˜ ë‹µë³€ì— ì˜¤ê°œë…ì´ ìˆë‹¤ë©´ ì˜¤ê°œë…ì— ë¶ˆë§Œì¡±ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ë°˜ë¡€ë¥¼ ì œì‹œí•˜ì—¬ í•™ìƒê³¼ ë¬¸ë‹µí•˜ì„¸ìš”. ë§Œì•½ 3íšŒ ì´ìƒ ë¬¸ë‹µí–ˆì§€ë§Œ ê³„ì† ì˜¤ê°œë…ì„ ê³ ìˆ˜í•˜ëŠ” ê²½ìš°ì—ëŠ” ê³¼í•™ì  ê°œë…ì„ ì•Œë ¤ì£¼ì„¸ìš”. ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ê°œë…ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    1) ìš´ë™ ë°©í–¥ìœ¼ë¡œ í˜ì´ ì‘ìš©í•´ì•¼ í•œë‹¤ê³  ìƒê°í•˜ëŠ” ê²½ìš°, í˜ì´ ì‘ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ì œì‹œ
    2) ì›ì‹¬ë ¥ì´ ì‹¤ì œ ì‘ìš©í•˜ëŠ” í˜ì´ë¼ê³  ìƒê°í•˜ëŠ” ê²½ìš°, ì›ì‹¬ë ¥ì€ ëˆ„ê°€ ì–´ë–»ê²Œ ì‘ìš©í•œ ê²ƒì¸ì§€ ìƒê°í•´ë³´ë„ë¡ ì œì‹œ

**ëŒ€í™” ìŠ¤íƒ€ì¼:**  
- í•™ìƒì´ ìƒˆë¡œ ìµíŒ ê°œë…ì„ í™œìš©í•˜ì—¬ ìì‹ ê°ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ê¸ì •ì ì´ê³  ê²©ë ¤í•˜ëŠ” íƒœë„ë¡œ ëŒ€í™”í•˜ì„¸ìš”.  
- ì—´ë¦° ì§ˆë¬¸ì„ í†µí•´ í•™ìƒì´ ìì‹ ì˜ ë‹µë³€ì„ ìŠ¤ìŠ¤ë¡œ ê²€í† í•˜ê³  ë°˜ì„±í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.  
- í•™ìƒì´ ìµíŒ ê°œë…ì´ ë¬¸ì œ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ ëª…í™•íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë•ë˜, ì§€ë‚˜ì¹œ ì•”ì‹œë¥¼ ì£¼ì§€ ë§ˆì„¸ìš”.

**í•µì‹¬ ì§ˆë¬¸ ìœ ë„:**  
- "ë‹¬ì´ ì¼ì •í•œ ê¶¤ë„ë¥¼ ë”°ë¼ ì›€ì§ì´ê¸° ìœ„í•´ ì–´ë–¤ í˜ì´ í•„ìš”í• ê¹Œìš”?"
- "ë‹¬ì´ ì¼ì •í•œ ê¶¤ë„ë¥¼ ë”°ë¼ ì›€ì§ì´ê¸° ìœ„í•œ í˜ì€ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì‘ìš©í•´ì•¼ í• ê¹Œìš”?"
- "ë‹¬ì— ì‘ìš©í•˜ëŠ” í˜ì€ ëˆ„ê°€ ì–´ë–»ê²Œ ì‘ìš©í•œ ê±´ê°€ìš”?
- "ë§Œì•½ ë‹¬ì— ì‘ìš©í•˜ëŠ” í˜ì´ ì—†ë‹¤ë©´, ë‹¬ì€ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¼ ê²ƒì´ë¼ê³  ìƒê°í•˜ì‹œë‚˜ìš”?"  
- "ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ê°œë…ì„ í™œìš©í•´ì„œ ë‹¬ì˜ ìš´ë™ì„ ì„¤ëª…í•´ ì£¼ì‹œê² ì–´ìš”?"  

**ì¤‘ìš” ê°œë…ì„ ìœ ë„í•˜ëŠ” ê³¼ì •:**  
- í•™ìƒì´ 'ë“±ì† ì›ìš´ë™'ì˜ ê°œë…ì„ ë‹¬ì˜ ìš´ë™ì— ì ìš©í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.  
- ë‹¬ì˜ ê³µì „ì— ëŒ€í•´ ë…¼ì˜í•˜ë©´ì„œ, êµ¬ì‹¬ë ¥(ì§€êµ¬ê°€ ì‘ìš©í•˜ëŠ” ì¤‘ë ¥)ì´ ë‹¬ì˜ ìš´ë™ì„ ìœ ì§€í•˜ëŠ” ë° í•„ìš”í•œ ì´ìœ ë¥¼ í•™ìƒì´ ìŠ¤ìŠ¤ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.  
- í•„ìš”í•˜ë‹¤ë©´ ì‹¤ìƒí™œì˜ ìœ ì‚¬í•œ ì˜ˆë¥¼ ì œì‹œí•˜ì—¬ í•™ìƒì´ ê°œë…ì„ ëª…í™•íˆ í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ì„¸ìš”.  

**ëŒ€í™” ë§ˆë¬´ë¦¬:**  
í•™ìƒì´ ìì‹ ì˜ ë‹µë³€ì„ ì„±ì°°í•˜ë„ë¡ ë•ê³ , ìƒˆë¡œ ìµíŒ ê°œë…ì´ ë¬¸ì œ í’€ì´ì— ì–´ë–»ê²Œ ì ìš©ë˜ì—ˆëŠ”ì§€ ë°˜ì„±ì ìœ¼ë¡œ ìƒê°í•˜ê²Œ í•˜ì„¸ìš”.  
- ì˜ˆ: "ì§€ê¸ˆê¹Œì§€ ë¬¸ë‹µí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ë‹¬ì´ ê³µì „ê¶¤ë„ë¥¼ ê³„ì† ìœ ì§€í•˜ë©´ì„œ ëŒ ìˆ˜ ìˆëŠ” ì´ìœ ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.""",
            ),
            # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, history ê°€ MessageHistory ì˜ key ê°€ ë¨
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}"),  # ì‚¬ìš©ì ì…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©
        ]
    )

    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
    # ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name=model_name, temperature=0, openai_api_key = st.session_state.api_key)
    
    # ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±
    with_message_history = (
        RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±
            prompt | llm,
            get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
            input_messages_key="input",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤
            history_messages_key="history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
        )
    )
    chain = with_message_history | StrOutputParser()

    return chain


# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ğŸ¤– AIíŠœí„°ì—ê²Œ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ì²´ì¸ ìƒì„±
if "application_chain" not in st.session_state:
    st.session_state["application_chain"] = create_chain()

chain = st.session_state["application_chain"]

# ì‚¬ì´ë“œë°”ì— relevance scoreë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ ìƒì„±
if "relevance_score" not in st.session_state:
    st.session_state["relevance_score"] = 0

# ì‚¬ì´ë“œë°”ì— ì ìˆ˜ í‘œì‹œ
with st.sidebar:
    st.header("ëª¨ë²” ë‹µì•ˆê³¼ì˜ ì¼ì¹˜ìœ¨")
    # ì ìˆ˜ í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤ì„ ë¯¸ë¦¬ ìƒì„±
    score_container = st.empty()
    score_text_container = st.empty()

if len(st.session_state["messages_application"]) == 0:
    init_user_input = f"""ë‹¤ìŒì˜ "ëŒ€í™” ì‹œì‘ ì œì•ˆ" ì¤‘ í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì‹œì‘í•´ì¤˜. ë‚˜ëŠ” ê·¸ ëŒ€í™”ì— ë§ì¶°ì„œ ë„ˆê°€ ë‚¸ ë¬¸ì œë¥¼ í•´ê²°í•´ë³¼ê²Œ.  
ë‹¤ë¥¸ ì–˜ê¸°ë¥¼ í•˜ì§€ ë§ê³  ì˜¤ë¡œì§€ 'ì§ˆë¬¸'ë§Œ ì œì‹œí•˜ë©´ì„œ ë„ˆì˜ ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•´ì¤˜. ë‹¤ì‹œ í•œë²ˆ ì–˜ê¸°í•˜ì§€ë§Œ ì²˜ìŒì—ëŠ” 'ì§ˆë¬¸'ë§Œ ì–˜ê¸°í•˜ëŠ” ê±°ì•¼ ë‹¤ë¥¸ ë‚´ìš©ì€ ì „í˜€ ì—†ì´.

**ëŒ€í™” ì‹œì‘ ì œì•ˆ:**  
- {applyingQuestion}  
  ìƒˆë¡œ ë°°ìš´ ê°œë…ì„ í™œìš©í•´ì„œ ì„¤ëª…í•´ ë³´ì‹¤ë˜ìš”?
- ë°©ê¸ˆ ë°°ìš´ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ë´…ì‹œë‹¤.  
  {applyingQuestion}"""
    
    response = chain.stream(
        {"input": init_user_input},
        # ì„¤ì • ì •ë³´ë¡œ ì„¸ì…˜ ID "abc123"ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        config={"configurable": {"session_id": "abc123"}},
    )
    with st.chat_message("assistant"):
        # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
        container = st.empty()

        ai_answer = ""
        for token in response:
            ai_answer += token
            container.markdown(ai_answer)
        
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
    add_message("assistant", ai_answer)

else:
    if user_input:
        # ì‚¬ìš©ìì˜ ì…ë ¥
        st.chat_message("user").write(user_input)
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = chain.stream(
            {"input": user_input},
            # ì„¤ì • ì •ë³´ë¡œ ì„¸ì…˜ ID "abc123"ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
            config={"configurable": {"session_id": "abc123"}},
        )
        
        # relevance score ê³„ì‚° ë° ì—…ë°ì´íŠ¸
        try:
            score = float(relevance_check(applyingModeledAnswer))
            st.session_state["relevance_score"] = score
            
            # ì ìˆ˜ì— ë”°ë¼ ë‹¤ë¥¸ ìƒ‰ìƒì˜ í”„ë¡œê·¸ë ˆìŠ¤ ë°” í‘œì‹œ
            if score >= 80:
                st.markdown("""
                    <style>
                        .stProgress > div > div {
                            background-color: blue;
                        }
                    </style>""", 
                    unsafe_allow_html=True
                )
            else:
                st.markdown("""
                    <style>
                        .stProgress > div > div {
                            background-color: red;
                        }
                    </style>""", 
                    unsafe_allow_html=True
                )
            
            score_container.progress(score / 100)
            score_text_container.write(f"ğŸ¯ {score:.1f}ì ")
            
            # 80% ì´ìƒ ë‹¬ì„± ì‹œ ì¶•í•˜ íš¨ê³¼ì™€ ë§ˆë¬´ë¦¬ ë©”ì‹œì§€
            if score >= 80:
                st.balloons()
                with st.chat_message("assistant"):
                    st.write("ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ì¶©ë¶„íˆ ì˜ ì´í•´í•˜ì…¨ë„¤ìš”. ì´ì œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                # ëŒ€í™” ì¢…ë£Œë¥¼ ìœ„í•œ í”Œë˜ê·¸ ì„¤ì •
                st.session_state["conversation_completed"] = True
            
        except (ValueError, TypeError) as e:
            st.error(f"ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        
        # ëŒ€í™”ê°€ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ AI ì‘ë‹µ ìƒì„±
        if not st.session_state.get("conversation_completed", False):
            # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            response = chain.stream(
                {"input": user_input},
                config={"configurable": {"session_id": "abc123"}},
            )
            
            with st.chat_message("assistant"):
                container = st.empty()
                ai_answer = ""
                for token in response:
                    ai_answer += token
                    container.markdown(ai_answer)
            
            # AI ë‹µë³€ë„ ëŒ€í™”ê¸°ë¡ì— ì €ì¥
            add_message("user", user_input)
            add_message("assistant", ai_answer)
